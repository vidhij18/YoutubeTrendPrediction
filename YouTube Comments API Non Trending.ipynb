{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our credentials set up, we can now start collecting comments! Weâ€™ll first build the service for calling the YouTube API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apiclient.discovery import build\n",
    "\n",
    "def build_service(filename):\n",
    "    with open(filename) as f:\n",
    "        key = f.readline()\n",
    "\n",
    "    YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "    YOUTUBE_API_VERSION = \"v3\"\n",
    "    return build(YOUTUBE_API_SERVICE_NAME,\n",
    "                 YOUTUBE_API_VERSION,\n",
    "                 developerKey=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = build_service('apikey.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('archive/US_youtube_trending_data.csv')\n",
    "df = pd.read_csv('ULTIMATEFINALData.csv')\n",
    "df = df.tail(2000)\n",
    "videoIdList = df['video_id'].tolist()\n",
    "trendingDateList = df['trending_date'].tolist()\n",
    "# videoIdList\n",
    "# trendingDateList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videoIdList = ['9bZkp7q19f0']\n",
    "# videoIdList = ['3C66w5Z0ixs','M9Pmf9AB4Mo','J78aPJ3VyNs','kXLn3HkpjaA','VIUo6yapDbc','w-aidBdvZo8','uet14uf9NsE',\n",
    "#                'ua4QMFQATco','SnsPZj91R7E','SsWHMAhshPQ','49Z6Mv4_WCA','nt3VVyv5pxQ','I6hswz4rIrU','W7VK4DUHvKU',\n",
    "#                'W9Aen8hG20Y','BNeDH6UTmXw','6TIsR_7nrNc','gPdUslndvVI','GTp-0S82guE']\n",
    "comment_count = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse for the whole list of youtube videos\n",
    "\tfor each video\n",
    "\t\tmake a service call for 100 responses of top comments\n",
    "\t\tstore the data in list\n",
    "\t\t\n",
    "\t\tmake the service call 9 time for 900 more responses\n",
    "\t\tstore in list\n",
    "\t\t\n",
    "\t\tcreate dataframe of lists\n",
    "\t\twrite in csv, file name comment id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "def fetch_comments_teratively(videoId, comment_count):\n",
    "    comment, comment_id, reply_count, like_count, published_date = [], [], [], [], []\n",
    "    first_response = True\n",
    "#     video_trending_date = trendingDateList[videoIdList.index(videoId)]\n",
    "    if (first_response):\n",
    "\n",
    "        response = service.commentThreads().list(\n",
    "            part='snippet',\n",
    "            maxResults=100,\n",
    "            textFormat='plainText',\n",
    "            order='time',\n",
    "            videoId=videoId\n",
    "        ).execute()\n",
    "        first_response = False\n",
    "    \n",
    "#     comment, comment_id, reply_count, like_count = add_data_to_lists(response, comment, comment_id, reply_count, like_count)\n",
    "    for item in response['items']:\n",
    "        delta = 2\n",
    "        if(delta <= 3 and delta >= 0):\n",
    "            if(detect(item['snippet']['topLevelComment']['snippet']['textDisplay']) == 'en'):\n",
    "                comment.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "                comment_id.append(item['snippet']['topLevelComment']['id'])\n",
    "                reply_count.append(item['snippet']['totalReplyCount'])\n",
    "                like_count.append(item['snippet']['topLevelComment']['snippet']['likeCount'])\n",
    "                published_date.append(item['snippet']['topLevelComment']['snippet']['publishedAt'])\n",
    "    \n",
    "#     print(len(comment))\n",
    "    while(len(comment) < comment_count and len(comment) >= 100):\n",
    "        try:\n",
    "            if(response['nextPageToken'] != None):\n",
    "#                 print(response['nextPageToken'])\n",
    "                response = service.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    maxResults=100,\n",
    "                    textFormat='plainText',\n",
    "                    order='time',\n",
    "                    videoId=videoId,\n",
    "                    pageToken=response['nextPageToken']\n",
    "                ).execute()\n",
    "\n",
    "                for item in response['items']:\n",
    "                    delta = 2\n",
    "                    if(delta <= 3 and delta >= 0):\n",
    "                        if(detect(item['snippet']['topLevelComment']['snippet']['textDisplay']) == 'en'):\n",
    "                            comment.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "                            comment_id.append(item['snippet']['topLevelComment']['id'])\n",
    "                            reply_count.append(item['snippet']['totalReplyCount'])\n",
    "                            like_count.append(item['snippet']['topLevelComment']['snippet']['likeCount'])\n",
    "                            published_date.append(item['snippet']['topLevelComment']['snippet']['publishedAt'])\n",
    "        except:\n",
    "            print(\"Cannot get NextPageToken for interation and Video Id\", i,videoId)\n",
    "            break\n",
    "    print(videoId,len(comment))\n",
    "    first_response = True\n",
    "    return comment, comment_id, reply_count, like_count, published_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(videoIdList)):\n",
    "    try:\n",
    "        comment, comment_id, reply_count, like_count, published_date = fetch_comments_teratively(videoIdList[i], comment_count)\n",
    "        comments = pd.DataFrame(list(zip(comment, comment_id, reply_count, like_count, published_date)),\n",
    "                       columns =['comment', 'comment_id', 'reply_count', 'like_count', 'published_date'])\n",
    "        if not comments.empty:\n",
    "            comments.to_csv(r\"comment_data/\"+videoIdList[i]+\".csv\", index=False)\n",
    "    except Exception as e:\n",
    "        print(\"Failed on this video, trying next...\", videoIdList[i])\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881\n"
     ]
    }
   ],
   "source": [
    "#test code\n",
    "print(len(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#test code\n",
    "from langdetect import detect\n",
    "try:\n",
    "    response = service.commentThreads().list(\n",
    "                part='snippet',\n",
    "                maxResults=100,\n",
    "                textFormat='plainText',\n",
    "                order='time',\n",
    "                videoId='7wrxP2PPzLA'\n",
    "            ).execute()\n",
    "\n",
    "    response_list = []\n",
    "\n",
    "    for item in response['items']:\n",
    "#         if(detect(item['snippet']['topLevelComment']['snippet']['textDisplay']) == 'en'):\n",
    "            response_list.append(item)\n",
    "#             print(item)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "# rawdata_df = pd.DataFrame(item, columns=[\"Commentdata\"])\n",
    "# rawdata_df.to_csv(r\"comment_data/rawdata_comments.csv\", index=False)\n",
    "print(len(response_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=1b44b55095456b4ece292113164d297567a0fa17d852df818d156c4f72f8827f\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
